{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import re\n",
    "import numpy as np\n",
    "from docx.shared import Pt\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_toc_hierarchy(doc):\n",
    "    toc_ind = [ind for ind,p in enumerate(doc.paragraphs) if p.text == 'TABLE OF CONTENTS']\n",
    "    toc = [p.text for p in doc.paragraphs[toc_ind[0]:] if bool(re.search(r'\\t+\\d', p.text))]\n",
    "\n",
    "    titles = []\n",
    "    subtitles = []\n",
    "    # page_ind = []\n",
    "    section = []\n",
    "    sub_section = []\n",
    "    for line in toc:\n",
    "        split_line = line.split('\\t')\n",
    "        if len(split_line) == 3:\n",
    "            sec = float(split_line[0])\n",
    "            section.append(sec)\n",
    "            if (sec % 1) == 0:\n",
    "                titles.append(split_line[-2].strip())\n",
    "                # page_ind.append(int(split_line[-1]))\n",
    "            else:\n",
    "                subtitles.append(split_line[-2].strip())\n",
    "                sub_section.append(titles[int(sec)-1])\n",
    "        else:\n",
    "            titles.append(split_line[0])\n",
    "            \n",
    "\n",
    "    index = np.array([ind for ind, paragraph in enumerate(doc.paragraphs) if paragraph.text.strip() in titles or paragraph.text.strip() in subtitles])\n",
    "    data_ind = {}\n",
    "    data = {k: {} for k in titles}\n",
    "    data_ind[0] = \"Title Page\"\n",
    "    data_ind[toc_ind[0]] = \"TABLE OF CONTENTS\"\n",
    "    for start,end in zip(index, np.append(index[1:], len(doc.paragraphs))):\n",
    "        key = [y.strip() for x,y in zip(subtitles, sub_section) if doc.paragraphs[start].text.strip() in x]\n",
    "        \n",
    "        if len(key) > 0:\n",
    "            txt = \" \".join([p.text for p in doc.paragraphs[start+1:end]])\n",
    "            data[key[0]][doc.paragraphs[start].text.strip()] = txt\n",
    "        data_ind[start] = doc.paragraphs[start].text.strip()\n",
    "    \n",
    "    return data_ind, data, toc_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_toc(doc):\n",
    "    # potentially want to add functionality to figure out subheadings - dict in dict, json?, \n",
    "    toc_ind = [ind for ind,p in enumerate(doc.paragraphs) if p.text == 'TABLE OF CONTENTS']\n",
    "    toc = [p.text for p in doc.paragraphs[toc_ind[0]:] if bool(re.search(r'\\t+\\d', p.text))]\n",
    "\n",
    "    titles = []\n",
    "    page_ind = []\n",
    "    for line in toc:\n",
    "        split_line = line.split('\\t')\n",
    "        # line_ind.append(float(split_line[0]))\n",
    "        titles.append(split_line[-2].strip())\n",
    "        page_ind.append(int(split_line[-1]))\n",
    "\n",
    "\n",
    "    index = np.array([ind for ind, paragraph in enumerate(doc.paragraphs) if paragraph.text.strip() in titles])\n",
    "\n",
    "    data = {}\n",
    "    data_ind = {}\n",
    "    data_ind[\"Title Page\"] = 0\n",
    "    data_ind[\"TABLE OF CONTENTS\"] = toc_ind[0]\n",
    "    for start,end in zip(index, np.append(index[1:], len(doc.paragraphs))):\n",
    "        data[doc.paragraphs[start].text.strip()] = \" \".join([p.text for p in doc.paragraphs[start+1:end]])\n",
    "        data_ind[doc.paragraphs[start].text.strip()] = start\n",
    "    return data_ind, data, toc_ind\n",
    "\n",
    "def find_tables(doc):\n",
    "    para_count = 0\n",
    "    table_loc = []\n",
    "    tables = []\n",
    "    for p in doc.iter_inner_content():\n",
    "        if isinstance(p, docx.text.paragraph.Paragraph):\n",
    "            para_count += 1\n",
    "        elif isinstance(p, docx.table.Table):\n",
    "            table_loc.append(para_count)\n",
    "            tables.append(p)\n",
    "    \n",
    "    return table_loc, tables\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_placeholders(doc):\n",
    "    placeholders = []\n",
    "    placeholders_ind = []\n",
    "    for paragraph_index, paragraph in enumerate(doc.paragraphs):\n",
    "        if '{' in paragraph.text:\n",
    "            start_index = paragraph.text.find('{')+1\n",
    "            end_index = paragraph.text.find('}')\n",
    "            \n",
    "            placeholders.append(paragraph.text[start_index:end_index])\n",
    "            placeholders_ind.append(paragraph_index)\n",
    "\n",
    "    return placeholders, placeholders_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_table_after(table, paragraph):\n",
    "    tbl, p = table._tbl, paragraph._p\n",
    "    p.addnext(tbl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = \"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/Example Proposals/LXN_02_13MAY21_CCI_R1.docx\"\n",
    "doc_path = \"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/Example Proposals/MGB_02_12DEC23_IVIS.docx\"\n",
    "edit_path = \"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/PRX_05_20APR23_ITCH_R3.1_test2.docx\"\n",
    "report_path = \"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/PRX_05 - Study Report_12132023.docx\"\n",
    "template_path = \"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/Becca_Template.docx\"\n",
    "original = docx.Document(doc_path)\n",
    "template = docx.Document(template_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "place, ind = find_all_placeholders(template)\n",
    "# Load document\n",
    "data_ind, data, toc_ind = parse_toc(original) \n",
    "title_page =[(ind, p.text) for ind, p in enumerate(original.paragraphs) if ind < toc_ind[0] and p.text.strip() != '' ]    \n",
    "table_loc, table = find_tables(original)\n",
    "\n",
    "# Identify key points\n",
    "title = title_page[0][1]\n",
    "study_num = [ (x[0], x[1].split(': ')[-1]) for x in title_page if 'Project Quotation' in x[1]][0]\n",
    "client = [ (x[0], x[1].split('Prepared for ')[-1]) for x in title_page if 'Prepared for' in x[1]][0]\n",
    "t = date.today().strftime(\"%B %d, %Y\")\n",
    "roughPM = \" \".join(data['Project Manager'].split('proposes ')[-1].split(\" \")[0:2])\n",
    "roughPC = \" \".join(data['Project Coordinator:'].split('proposes ')[-1].split(\" \")[0:2])\n",
    "replace_str = {'Study Num': study_num[1], 'Title': title, 'Client': client[1], 'Date':t, 'Project Manager': roughPM, 'Project Coordinator': roughPC}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ind = np.array(list(data_ind.values()))\n",
    "add_factor = 0\n",
    "for p,i in zip(place, ind):\n",
    "    if p in replace_str.keys():\n",
    "        old_string = '{'+p+'}'\n",
    "        new_string = template.paragraphs[i+add_factor].text.replace(old_string, replace_str[p])\n",
    "        template.paragraphs[i+add_factor].text = new_string\n",
    "    elif p == 'Table 1':\n",
    "        paragraph = template.paragraphs[i+add_factor]  # however you get this paragraph\n",
    "        # table = template.add_table(table)\n",
    "        move_table_after(table[0], paragraph)\n",
    "        template.paragraphs[i+add_factor].clear()\n",
    "    elif p in data_ind.keys() :\n",
    "        start = data_ind[p]\n",
    "        end = all_ind[np.argwhere(all_ind == start)[0][0]+1]\n",
    "        for x in range(end-1, start, -1):\n",
    "            template.paragraphs[i+add_factor].insert_paragraph_before(original.paragraphs[x].text, original.paragraphs[x].style)\n",
    "        \n",
    "        add_factor = add_factor + len(range(start,end)) - 1\n",
    "        template.paragraphs[i+add_factor].clear()\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_loc, temp_tables = find_tables(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_len = len(temp_tables[0].rows)\n",
    "col_len = len(temp_tables[0].columns)\n",
    "for r in range(row_len):\n",
    "    for c in range(col_len):\n",
    "        if '{' in temp_tables[0].cell(r,c).text:\n",
    "            start_index = temp_tables[0].cell(r,c).text.find('{')+1\n",
    "            end_index = temp_tables[0].cell(r,c).text.find('}')\n",
    "            \n",
    "            placeholder = (temp_tables[0].cell(r,c).text[start_index:end_index])\n",
    "\n",
    "            if placeholder in replace_str.keys():\n",
    "                old_string = '{'+placeholder+'}'\n",
    "                new_string = temp_tables[0].cell(r,c).text.replace(old_string, replace_str[placeholder])\n",
    "                temp_tables[0].cell(r,c).text = new_string\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = template.sections[0]\n",
    "header = section.header\n",
    "for paragraph in header.paragraphs:\n",
    "    starts =[m.start() for m in re.finditer(r\"{\",paragraph.text)]\n",
    "    ends = [m.start() for m in re.finditer(r\"}\",paragraph.text)]\n",
    "    \n",
    "    for s,e in zip(starts, ends):\n",
    "        placeholder = (paragraph.text[s+1:e])\n",
    "        if placeholder in replace_str.keys():\n",
    "            old_string = '{'+placeholder+'}'\n",
    "            new_string = paragraph.text.replace(old_string, replace_str[placeholder])\n",
    "            paragraph.text = new_string\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "template.save(edit_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = template.sections[0].header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "    iterate through paragraphs in 'original' document\n",
    "    using the index of the current paragraph - identify what section of the document currently iterating through\n",
    "        identify paragraphs to be deleted\n",
    "        identify paragraphs to be included\n",
    "        identify paragraphs to be edited\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data to scrape from a proposal:\n",
    "- Project title\n",
    "- Client name\n",
    "- date of issue\n",
    "- date(s) of reissue\n",
    "- project ID\n",
    "- methods used \n",
    "    - reference experiment abbreviation list\n",
    "- people included\n",
    "    - project manager\n",
    "    - project coordinator\n",
    "    - client coordinator\n",
    "\n",
    "Data to scrape from report\n",
    "- ^ match above\n",
    "- date of report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
