{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docxtpl import DocxTemplate\n",
    "from docx import Document, table, text\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "# from docx.shared import Pt\n",
    "from datetime import date\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_toc(doc):\n",
    "    # potentially want to add functionality to figure out subheadings - dict in dict, json?, \n",
    "    toc_ind = [ind for ind,p in enumerate(doc.paragraphs) if p.text == 'TABLE OF CONTENTS']\n",
    "    toc = [p.text for p in doc.paragraphs[toc_ind[0]:] if bool(re.search(r'\\t+\\d', p.text))]\n",
    "\n",
    "    titles = []\n",
    "    page_ind = []\n",
    "    for line in toc:\n",
    "        split_line = line.split('\\t')\n",
    "        # line_ind.append(float(split_line[0]))\n",
    "        titles.append(split_line[-2].strip())\n",
    "        page_ind.append(int(split_line[-1]))\n",
    "\n",
    "\n",
    "    index = np.array([ind for ind, paragraph in enumerate(doc.paragraphs) if paragraph.text.strip() in titles])\n",
    "\n",
    "    data = {}\n",
    "    data_ind = {}\n",
    "    data_ind[\"Title Page\"] = 0\n",
    "    data_ind[\"TABLE OF CONTENTS\"] = toc_ind[0]\n",
    "    for start,end in zip(index, np.append(index[1:], len(doc.paragraphs))):\n",
    "        key = doc.paragraphs[start].text.strip()\n",
    "        key = key.replace(' ', '_')\n",
    "\n",
    "        data[key] = \" \".join([p.text for p in doc.paragraphs[start+1:end]])\n",
    "        data_ind[key] = start\n",
    "    \n",
    "    sortem = sorted(data_ind.items(), key=lambda x: x[1]) \n",
    "    data_ind = {tup[0]:tup[1] for tup in sortem}\n",
    "    return data_ind , data\n",
    "\n",
    "def find_tables(doc):\n",
    "    para_count = 0\n",
    "    table_loc = []\n",
    "    tables = []\n",
    "    for p in doc.iter_inner_content():\n",
    "       \n",
    "        if isinstance(p, table.Table):\n",
    "            table_loc.append(para_count)\n",
    "            tables.append(p)\n",
    "        elif isinstance(p, text.paragraph.Paragraph):\n",
    "            para_count += 1\n",
    "    \n",
    "    return table_loc, tables\n",
    "\n",
    "\n",
    "def move_table_after(table, paragraph):\n",
    "    tbl, p = table._tbl, paragraph._p\n",
    "    p.addnext(tbl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_proposal(input_path, template_path, save_path = None, save = True):\n",
    "    \n",
    "    # Load proposal and template docx\n",
    "    input = Document(input_path)\n",
    "    template = DocxTemplate(template_path)\n",
    "\n",
    "    # Pull data from proposal (input)\n",
    "    data_ind,data = parse_toc(input) \n",
    "    title_page =[(ind, p.text) for ind, p in enumerate(input.paragraphs) if ind < data_ind['TABLE OF CONTENTS'] and p.text.strip() != '' ]    \n",
    "    table_loc, table = find_tables(input)\n",
    "\n",
    "    # Pull placeholders from template\n",
    "    place = template.get_undeclared_template_variables()\n",
    "    replace = {k : None for k in place}\n",
    "\n",
    "    # Identify key points, update replace dict\n",
    "    replace['title'] = title_page[0][1]\n",
    "    replace['study_num'] = [ x[1].split(': ')[-1] for x in title_page if 'Project Quotation' in x[1]][0]\n",
    "    replace['client'] = [x[1].split('Prepared for ')[-1] for x in title_page if 'Prepared for' in x[1]][0]\n",
    "    replace['t'] = date.today().strftime(\"%B %d, %Y\")\n",
    "    replace['description'] = input.sections[0].header.paragraphs[0].text.split('\\t')[0]\n",
    "\n",
    "    # Roughly identify the project manager and coordinator\n",
    "    m = [x for x in data.keys() if 'Project_Manager' in x]\n",
    "    if len(m) > 0:\n",
    "        replace['pm'] = \" \".join(data[m[0]].split('proposes ')[-1].split(\" \")[0:2])\n",
    "\n",
    "    c = [x for x in data.keys() if 'Project_Coordinator' in x]\n",
    "    if len(c) > 0:\n",
    "        replace['pc'] = \" \".join(data[c[0]].split('proposes ')[-1].split(\" \")[0:2])\n",
    "\n",
    "    # Replace '&' in strings to prevent jinga2 errors\n",
    "    for k,v in replace.items():\n",
    "        if v is not None and '&' in v:\n",
    "            replace[k] = v.replace(\"&\",\"and\")\n",
    "\n",
    "    \n",
    "    # For section replacement, iterate through and pull paragraphs from proposal\n",
    "    all_ind = np.array(list(data_ind.values()))\n",
    "    for p in place:\n",
    "        if p in data_ind.keys():\n",
    "            print(p)\n",
    "            start = data_ind[p]\n",
    "            end = all_ind[np.argwhere(all_ind == start)[0][0]+1]\n",
    "            print(p+\": \"+str(start)+\":\"+str(end))\n",
    "            sd1 = template.new_subdoc()\n",
    "            \n",
    "            # if (start+1 == end):\n",
    "            #     replace[p] = None\n",
    "            # else:\n",
    "            for x in range(start+1, end, 1):\n",
    "                sd1.add_paragraph(input.paragraphs[x].text, input.paragraphs[x].style)\n",
    "                if table_loc[0]-1 == x:\n",
    "                    move_table_after(table[0], sd1.paragraphs[-1])\n",
    "\n",
    "            replace[p] = sd1\n",
    "    \n",
    "    path, fn = os.path.split(input_path)\n",
    "    name, ext = os.path.splitext(fn)\n",
    "    new_fn = name + '_REPORT' + ext\n",
    "\n",
    "\n",
    "    if save_path is None:\n",
    "        save_path = os.path.join(path, new_fn)\n",
    "    else:\n",
    "        save_path = os.path.join(save_path, new_fn)\n",
    "\n",
    "\n",
    "    template.render(replace)\n",
    "\n",
    "    if save:\n",
    "        template.save(save_path)\n",
    "\n",
    "    return template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction_and_Background\n",
      "Introduction_and_Background: 47:52\n",
      "Animal_Description\n",
      "Animal_Description: 74:82\n",
      "Housing_and_Feeding\n",
      "Housing_and_Feeding: 82:91\n",
      "Methods\n",
      "Methods: 118:127\n",
      "References\n",
      "References: 127:132\n",
      "Design\n",
      "Design: 91:106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<docxtpl.template.DocxTemplate at 0x120f757d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_path = \"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/Report_Template_dxpt.docx\"\n",
    "# template = DocxTemplate(template_path)\n",
    "\n",
    "edit_path = \"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/Report_test.docx\"\n",
    "# input_path = \"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/Example Proposals/MGB_02_12DEC23_IVIS.docx\"\n",
    "# input_path = \"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/Example Proposals/VBI_01_19FEB20_EAE.docx\"\n",
    "input_path = \"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/Example Proposals/LXN_02_13MAY21_CCI_R1.docx\"\n",
    "# input_path = \"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/Test_Proposal/CYTK_05_16OCT23_HT.docx\"\n",
    "\n",
    "scrape_proposal(input_path, template_path,save= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Document(input_path)\n",
    "data_ind, data = parse_toc(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title Page': 0,\n",
       " 'TABLE OF CONTENTS': 20,\n",
       " 'Introduction_and_Background': 48,\n",
       " 'Background': 49,\n",
       " 'Abbreviations_used_in_this_proposal': 53,\n",
       " 'Study_Deliverables': 70,\n",
       " 'Marshall_Gerstein': 71,\n",
       " 'Melior': 83,\n",
       " 'Experimental_Procedures': 92,\n",
       " 'Animal_Description': 93,\n",
       " 'Housing_and_Feeding': 101,\n",
       " 'Design': 110,\n",
       " 'General_Operational_Terms': 123,\n",
       " 'Methods': 134,\n",
       " 'Data_Analysis': 141,\n",
       " 'Terms_and_Conditions': 143,\n",
       " 'Pricing': 145,\n",
       " 'Terms': 149,\n",
       " 'Appendix_1:__Key_Personnel_for_this_project': 198,\n",
       " 'Project_Manager': 199,\n",
       " 'Project_Coordinator:': 213,\n",
       " 'Client_Management_Specialist:': 228,\n",
       " 'Appendix_2:____Melior_Discovery_Background_and_Operations': 241,\n",
       " 'Executive_Summary': 242,\n",
       " 'Melior_Discovery_Overview': 251,\n",
       " 'Facility': 255,\n",
       " 'Security,_Monitoring,_and_Backup_Capability': 258,\n",
       " 'Special_Licenses_/_Certificates': 263}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_dir = \"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/Test_Proposal\"\n",
    "report_dir = \"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/Test_Report\"\n",
    "\n",
    "current_proposals = os.listdir(proposal_dir)\n",
    "current_reports = os.listdir(report_dir)\n",
    "\n",
    "proposal_names = [x.split('.docx')[0] for x in current_proposals]\n",
    "report_names = [x.split('_REPORT.docx')[0] for x in current_reports]\n",
    "\n",
    "unmatched = [p for r,p in zip(proposal_names, current_proposals) if r not in report_names]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methods: 118:127\n",
      "Design: 91:106\n",
      "Housing_and_Feeding: 82:91\n",
      "Animal_Description: 74:82\n",
      "Introduction_and_Background: 47:52\n",
      "References: 127:132\n",
      "Methods: 116:140\n",
      "Design: 90:104\n",
      "Housing_and_Feeding: 81:90\n",
      "Animal_Description: 73:81\n",
      "Introduction_and_Background: 47:51\n"
     ]
    }
   ],
   "source": [
    "for u in unmatched:\n",
    "    proposal_path = os.path.join(proposal_dir, u)\n",
    "    scrape_proposal(proposal_path, template_path, save_path = report_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VBI_01_19FEB20_EAE.docx']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccakrall/miniconda3/envs/pra/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "assays = pd.read_excel(\"/Users/rebeccakrall/Desktop/Example for Proposal Report Automation/Assay Codes.xlsx\")\n",
    "list(assays['Assay'])\n",
    "\n",
    "lower_assay = [l.lower() for l in list(assays['Assay'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_list = []\n",
    "for paragraph in input.paragraphs:\n",
    "    for ass in list(assays['Assay']):\n",
    "        if ass.lower() in paragraph.text.lower():\n",
    "            assay_list.append(ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Platform', 'Platform']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assay_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Randomization', 'NN'), (':', ':'), ('animals', 'NNS'), ('will', 'MD'), ('be', 'VB'), ('assigned', 'VBN'), ('randomly', 'RB'), ('to', 'TO'), ('treatment', 'NN'), ('groups', 'NNS'), (';', ':'), ('animals', 'NNS'), ('will', 'MD'), ('be', 'VB'), ('distributed', 'VBN'), ('into', 'IN'), ('treatment', 'NN'), ('groups', 'NNS'), ('according', 'VBG'), ('to', 'TO'), ('post-surgery', 'JJ'), ('pre-dose', 'JJ'), ('responses', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(example_txt)\n",
    "tagged_words = pos_tag(tokens)\n",
    "\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rough_past_tense(txt):\n",
    "    # lemmatize to look for plural nouns as subjects, all verbs\n",
    "    # change tense of all future/present verbs to past\n",
    "    # look for md\n",
    "    words = txt.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(txt)\n",
    "tagged_words = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = [ind for (ind,word) in enumerate(tagged_words) if 'VB' in word[1]]\n",
    "wills = [ind for (ind,word) in enumerate(tagged_words) if 'will' in word[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will Be found starting at 3\n",
      "Will not/never be found at 12\n"
     ]
    }
   ],
   "source": [
    "# replace will be and will not be \n",
    "for w in wills:\n",
    "    if tagged_words[w+1][0] == 'be':\n",
    "        print(\"Will Be found starting at \"+str(w))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    elif ((tagged_words[w+1][0] == 'not') | (tagged_words[w+1][0] == 'never')) & (tagged_words[w+2][0] == 'be'):\n",
    "        print(\"Will not/never be found at \"+str(w))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"The Prism file can be provided to Marshall Gerstein upon request.  Additional post-hoc analyses can be conducted by Melior upon request for additional cost.  \"\n",
    "# txt = \"Bioluminescent imaging will be performed on all animals on at either 4, 8, and 24 hours after TA administration.  Animals will be anesthetized using isoflurane inhalation.  Imaging will be conducted using a Perkin Elmer IVIS® Lumina III LT according to manufactures recommendations and signal analyzed using Perkin Elmer Living Image® 4.7.4 software.   D-Luciferin at concentration of 15 mg/mL will be used for RO injection. The imaging time will be 1 min post- D-Luciferin injection. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pyinflect\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# txt = 'Randomization: animals will be assigned randomly to treatment groups. Animals will always be distributed into treatment groups according to post-surgery pre-dose responses.'\n",
    "# txt = \"Test articles will be provided as pre-formulated stock to Melior to be diluted in PBS ready to a concentration of 1µg / 20 µl.  Test articles will be administered IM injection into the caudal thigh muscle. IM injection volume will be 20 µl (=1 µg) per injections site into each of the left and right caudal thigh muscle.\"\n",
    "doc=nlp(txt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roots = [tok for tok in doc if tok.dep_ == \"ROOT\"]\n",
    "doc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:\n",
      "\tNOUN\n",
      "\tnsubjpass\n",
      "\tNN\n",
      "2\n",
      "can:\n",
      "\tAUX\n",
      "\taux\n",
      "\tMD\n",
      "3\n",
      "be:\n",
      "\tAUX\n",
      "\tauxpass\n",
      "\tVB\n",
      "4\n",
      "to:\n",
      "\tADP\n",
      "\tprep\n",
      "\tIN\n",
      "6\n",
      "upon:\n",
      "\tSCONJ\n",
      "\tprep\n",
      "\tIN\n",
      "9\n",
      ".:\n",
      "\tPUNCT\n",
      "\tpunct\n",
      "\t.\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for c in doc[5].children:\n",
    "    print(c.text+\":\")\n",
    "    print('\\t'+c.pos_)\n",
    "    print('\\t'+c.dep_)\n",
    "    print('\\t'+c.tag_)\n",
    "    print(c.i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot delete function call (2853796180.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[106], line 22\u001b[0;36m\u001b[0m\n\u001b[0;31m    del word_list(ind)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot delete function call\n"
     ]
    }
   ],
   "source": [
    "# def past_tense_root(word_list, root_verb):\n",
    "word_list = [tok.text for tok in doc]\n",
    "root_verb = doc[5]\n",
    "delete_inds = []\n",
    "for root_verb in roots:\n",
    "    children = root_verb.children\n",
    "    aux = [tok for tok in root_verb.children if 'aux' in tok.dep_]\n",
    "    subj = [tok for tok in root_verb.children if 'subj' in tok.dep_]\n",
    "\n",
    "    subj_plural = ['S' in tok.tag_ for tok in subj]\n",
    "    print(subj_plural[-1])\n",
    "    for a in aux:\n",
    "        if a.tag_ == \"MD\":\n",
    "            if subj_plural[-1]:\n",
    "                word_list[a.i] = \"were\"\n",
    "            else:\n",
    "                word_list[a.i] = \"was\"\n",
    "        else:\n",
    "            delete_inds.append(a.i)\n",
    "            \n",
    "for ind in delete_inds:\n",
    "    del word_list(ind)\n",
    "new_txt = \" \".join(word_list)\n",
    "# cases to account for - no subj identified \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Prism file was provided to Marshall Gerstein upon request .   Additional post - hoc analyses can were by Melior upon request for additional cost .  '"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_list_replace(list, old, new):\n",
    "    for ind,l in enumerate(list):\n",
    "        if l == old:\n",
    "            print(l)\n",
    "            print(old)\n",
    "            list[ind] = new\n",
    "    \n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bioluminescent', 'imaging', 'will', 'be', 'performed', 'on', 'all', 'animals', 'on', 'at', 'either', '4', ',', '8', ',', 'and', '24', 'hours', 'after', 'TA', 'administration', '.', ' ', 'Animals', 'will', 'be', 'anesthetized', 'using', 'isoflurane', 'inhalation', '.', ' ', 'Imaging', 'will', 'be', 'conducted', 'using', 'a', 'Perkin', 'Elmer', 'IVIS', '®', 'Lumina', 'III', 'LT', 'according', 'to', 'manufactures', 'recommendations', 'and', 'signal', 'analyzed', 'using', 'Perkin', 'Elmer', 'Living', 'Image', '®', '4.7.4', 'software', '.', '  ', 'D', '-', 'Luciferin', 'at', 'concentration', 'of', '15', 'mg', '/', 'mL', 'will', 'be', 'used', 'for', 'RO', 'injection', '.', 'The', 'imaging', 'time', 'will', 'be', '1', 'min', 'post-', 'D', '-', 'Luciferin', 'injection', '.']\n",
      "performed\n",
      "performed\n",
      "['Bioluminescent', 'imaging', 'will', 'be', 'performed', 'on', 'all', 'animals', 'on', 'at', 'either', '4', ',', '8', ',', 'and', '24', 'hours', 'after', 'TA', 'administration', '.', ' ', 'Animals', 'will', 'be', 'anesthetized', 'using', 'isoflurane', 'inhalation', '.', ' ', 'Imaging', 'will', 'be', 'conducted', 'using', 'a', 'Perkin', 'Elmer', 'IVIS', '®', 'Lumina', 'III', 'LT', 'according', 'to', 'manufactures', 'recommendations', 'and', 'signal', 'analyzed', 'using', 'Perkin', 'Elmer', 'Living', 'Image', '®', '4.7.4', 'software', '.', '  ', 'D', '-', 'Luciferin', 'at', 'concentration', 'of', '15', 'mg', '/', 'mL', 'will', 'be', 'used', 'for', 'RO', 'injection', '.', 'The', 'imaging', 'time', 'will', 'be', '1', 'min', 'post-', 'D', '-', 'Luciferin', 'injection', '.']\n",
      "anesthetized\n",
      "anesthetized\n",
      "['Bioluminescent', 'imaging', 'will', 'be', 'performed', 'on', 'all', 'animals', 'on', 'at', 'either', '4', ',', '8', ',', 'and', '24', 'hours', 'after', 'TA', 'administration', '.', ' ', 'Animals', 'will', 'be', 'anesthetized', 'using', 'isoflurane', 'inhalation', '.', ' ', 'Imaging', 'will', 'be', 'conducted', 'using', 'a', 'Perkin', 'Elmer', 'IVIS', '®', 'Lumina', 'III', 'LT', 'according', 'to', 'manufactures', 'recommendations', 'and', 'signal', 'analyzed', 'using', 'Perkin', 'Elmer', 'Living', 'Image', '®', '4.7.4', 'software', '.', '  ', 'D', '-', 'Luciferin', 'at', 'concentration', 'of', '15', 'mg', '/', 'mL', 'will', 'be', 'used', 'for', 'RO', 'injection', '.', 'The', 'imaging', 'time', 'will', 'be', '1', 'min', 'post-', 'D', '-', 'Luciferin', 'injection', '.']\n",
      "conducted\n",
      "conducted\n",
      "['Bioluminescent', 'imaging', 'will', 'be', 'performed', 'on', 'all', 'animals', 'on', 'at', 'either', '4', ',', '8', ',', 'and', '24', 'hours', 'after', 'TA', 'administration', '.', ' ', 'Animals', 'will', 'be', 'anesthetized', 'using', 'isoflurane', 'inhalation', '.', ' ', 'Imaging', 'will', 'be', 'conducted', 'using', 'a', 'Perkin', 'Elmer', 'IVIS', '®', 'Lumina', 'III', 'LT', 'according', 'to', 'manufactures', 'recommendations', 'and', 'signal', 'analyzed', 'using', 'Perkin', 'Elmer', 'Living', 'Image', '®', '4.7.4', 'software', '.', '  ', 'D', '-', 'Luciferin', 'at', 'concentration', 'of', '15', 'mg', '/', 'mL', 'will', 'be', 'used', 'for', 'RO', 'injection', '.', 'The', 'imaging', 'time', 'will', 'be', '1', 'min', 'post-', 'D', '-', 'Luciferin', 'injection', '.']\n",
      "used\n",
      "used\n",
      "['Bioluminescent', 'imaging', 'will', 'be', 'performed', 'on', 'all', 'animals', 'on', 'at', 'either', '4', ',', '8', ',', 'and', '24', 'hours', 'after', 'TA', 'administration', '.', ' ', 'Animals', 'will', 'be', 'anesthetized', 'using', 'isoflurane', 'inhalation', '.', ' ', 'Imaging', 'will', 'be', 'conducted', 'using', 'a', 'Perkin', 'Elmer', 'IVIS', '®', 'Lumina', 'III', 'LT', 'according', 'to', 'manufactures', 'recommendations', 'and', 'signal', 'analyzed', 'using', 'Perkin', 'Elmer', 'Living', 'Image', '®', '4.7.4', 'software', '.', '  ', 'D', '-', 'Luciferin', 'at', 'concentration', 'of', '15', 'mg', '/', 'mL', 'will', 'be', 'used', 'for', 'RO', 'injection', '.', 'The', 'imaging', 'time', 'will', 'be', '1', 'min', 'post-', 'D', '-', 'Luciferin', 'injection', '.']\n",
      "be\n",
      "be\n",
      "be\n",
      "be\n",
      "be\n",
      "be\n",
      "be\n",
      "be\n",
      "be\n",
      "be\n",
      "['Bioluminescent', 'imaging', 'will', 'was', 'performed', 'on', 'all', 'animals', 'on', 'at', 'either', '4', ',', '8', ',', 'and', '24', 'hours', 'after', 'TA', 'administration', '.', ' ', 'Animals', 'will', 'was', 'anesthetized', 'using', 'isoflurane', 'inhalation', '.', ' ', 'Imaging', 'will', 'was', 'conducted', 'using', 'a', 'Perkin', 'Elmer', 'IVIS', '®', 'Lumina', 'III', 'LT', 'according', 'to', 'manufactures', 'recommendations', 'and', 'signal', 'analyzed', 'using', 'Perkin', 'Elmer', 'Living', 'Image', '®', '4.7.4', 'software', '.', '  ', 'D', '-', 'Luciferin', 'at', 'concentration', 'of', '15', 'mg', '/', 'mL', 'will', 'was', 'used', 'for', 'RO', 'injection', '.', 'The', 'imaging', 'time', 'will', 'was', '1', 'min', 'post-', 'D', '-', 'Luciferin', 'injection', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pyinflect\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sent = txt\n",
    "doc=nlp(sent)\n",
    "\n",
    "roots = [tok for tok in doc if tok.dep_ == \"ROOT\"]\n",
    "word_list = [tok.text for tok in doc]\n",
    "print(word_list)\n",
    "for r in roots:\n",
    "    \n",
    "    aux = [x for x in r.children if 'aux' in x.dep_]\n",
    "    subj = [y.tag_ for y in r.children if 'subj' in y.dep_]\n",
    "\n",
    "    if len(aux) > 0 & len(subj) > 0:\n",
    "\n",
    "        if subj[-1] == \"NNS\":\n",
    "            word_list = word_list_replace(word_list, aux[0].text, 'were')\n",
    "            print(word_list)\n",
    "            # txt2 = txt2.replace(aux[0].text, 'were')\n",
    "        else:\n",
    "            print('singular')\n",
    "            word_list = word_list_replace(word_list, aux[0].text, 'was')\n",
    "            # txt2 =txt2.replace(aux[0].text+' ', 'was')\n",
    "            print(word_list)\n",
    "        \n",
    "        for a in aux[1:]:\n",
    "            word_list =word_list_replace(word_list, a.text, '')\n",
    "            print(word_list)\n",
    "                # txt2 = txt2.replace(a.text+ ' ', '')\n",
    "    \n",
    "    # txt2 = txt2.replace(r.text, r._.inflect('VBD'))\n",
    "    word_list = word_list_replace(word_list, r.text, r._.inflect('VBD'))\n",
    "    print(word_list)\n",
    "    txt2 = ' '.join(word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(a) for a in doc[4].rights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bioluminescent imaging will was performed on all animals on at either 4 , 8 , and 24 hours after TA administration .   Animals will was anesthetized using isoflurane inhalation .   Imaging will was conducted using a Perkin Elmer IVIS ® Lumina III LT according to manufactures recommendations and signal analyzed using Perkin Elmer Living Image ® 4.7.4 software .    D - Luciferin at concentration of 15 mg / mL will was used for RO injection . The imaging time will was 1 min post- D - Luciferin injection .'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compound\n",
      "compound\n",
      "compound\n",
      "nsubjpass\n",
      "aux\n",
      "auxpass\n",
      "ccomp\n",
      "prep\n",
      "prep\n",
      "advmod\n",
      "amod\n",
      "pobj\n",
      "punct\n",
      "appos\n",
      "cc\n",
      "conj\n",
      "punct\n",
      "npadvmod\n",
      "punct\n",
      "punct\n",
      "dep\n",
      "nummod\n",
      "amod\n",
      "nsubjpass\n",
      "acl\n",
      "nummod\n",
      "punct\n",
      "prep\n",
      "amod\n",
      "compound\n",
      "dobj\n",
      "prep\n",
      "advmod\n",
      "nummod\n",
      "compound\n",
      "pobj\n",
      "aux\n",
      "auxpass\n",
      "ROOT\n",
      "prep\n",
      "compound\n",
      "pobj\n",
      "punct\n",
      "dep\n",
      "nsubjpass\n",
      "aux\n",
      "auxpass\n",
      "ROOT\n",
      "prep\n",
      "compound\n",
      "pobj\n",
      "prep\n",
      "advmod\n",
      "advmod\n",
      "nummod\n",
      "compound\n",
      "pobj\n",
      "prep\n",
      "pobj\n",
      "punct\n"
     ]
    }
   ],
   "source": [
    "for tok in doc:\n",
    "    print(tok.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Randomization: animals will be assigned randomly to treatment groups. Animals will not be distributed into treatment groups according to post-surgery pre-dose responses.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT  --  root\n",
      "acl  --  clausal modifier of noun (adjectival clause)\n",
      "acomp  --  adjectival complement\n",
      "advcl  --  adverbial clause modifier\n",
      "advmod  --  adverbial modifier\n",
      "agent  --  agent\n",
      "amod  --  adjectival modifier\n",
      "appos  --  appositional modifier\n",
      "attr  --  attribute\n",
      "aux  --  auxiliary\n",
      "auxpass  --  auxiliary (passive)\n",
      "case  --  case marking\n",
      "cc  --  coordinating conjunction\n",
      "ccomp  --  clausal complement\n",
      "compound  --  compound\n",
      "conj  --  conjunct\n",
      "csubj  --  clausal subject\n",
      "csubjpass  --  clausal subject (passive)\n",
      "dative  --  dative\n",
      "dep  --  unclassified dependent\n",
      "det  --  determiner\n",
      "dobj  --  direct object\n",
      "expl  --  expletive\n",
      "intj  --  interjection\n",
      "mark  --  marker\n",
      "meta  --  meta modifier\n",
      "neg  --  negation modifier\n",
      "nmod  --  modifier of nominal\n",
      "npadvmod  --  noun phrase as adverbial modifier\n",
      "nsubj  --  nominal subject\n",
      "nsubjpass  --  nominal subject (passive)\n",
      "nummod  --  numeric modifier\n",
      "oprd  --  object predicate\n",
      "parataxis  --  parataxis\n",
      "pcomp  --  complement of preposition\n",
      "pobj  --  object of preposition\n",
      "poss  --  possession modifier\n",
      "preconj  --  pre-correlative conjunction\n",
      "predet  --  None\n",
      "prep  --  prepositional modifier\n",
      "prt  --  particle\n",
      "punct  --  punctuation\n",
      "quantmod  --  modifier of quantifier\n",
      "relcl  --  relative clause modifier\n",
      "xcomp  --  open clausal complement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccakrall/miniconda3/envs/pra/lib/python3.11/site-packages/spacy/glossary.py:20: UserWarning: [W118] Term 'predet' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n"
     ]
    }
   ],
   "source": [
    "for label in nlp.get_pipe(\"parser\").labels:\n",
    "    print(label, \" -- \", spacy.explain(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
